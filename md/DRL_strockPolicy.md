## 深度强化学习自动炒股
---
&emsp;&emsp;LSTM（监督学习）可以根据各种历史数据来预测未来的价格，判断股票涨还是跌，来协助做决策。

&emsp;&emsp;强化学习是机器学习的一个分支：
在决策时候采取的行动（Action）使最后的奖励最大化。
与LSTM预测未来数值不同，强化学习根据输入的状态（如当日开盘价，收盘价等），输出系列动作（例如：买进、持有、卖出），使得最后的收益最大化，实现自动交易。

### OpenAi Gym股票交易环境
#### 观测 Observation
策略网络观测的就是一只股票的各项参数，比如开盘价、收盘价、成交数量等。部分数值会是一个很大的数值。
观测的状态数据输入时，必须要进行归一化，变化到[-1,1]的区间内。

------

### 动作action
假设交易共有买入、卖出和保持3中操作，定义动作（action）为长度 $2$ 的数据

- action[0] 为操作类型；
- action[1] 表示买入或卖出百分比；

| 动作类型 action[0] | 说明 |
| :---- | :--- |
| 1 | 买入 `action[1]` |
| 2 | 卖出 `action[1]` |
| 3 | 保持  |

### 强化学习在深度学习中的学习方法
- Critic-only方法：仅评论家方法的主要限制是它仅适用于离散且有限的状态和动作空间，这对于大型股票投资组合不切实际，因为股票是连续的
- Actor-only方法：这里的想法是Agent直接学习一个策略。策略是一个概率分布，本质上是给定状态的策略，即采取允许的动作的可能性。Actor-only方法可以处理连续动作空间环境
- Actor-Critic方法：演员-评论家方法最近已应用于金融领域。这个想法是同时更新代表策略的actor网络和代表价值函数的critic网络。评论家估计价值函数，而演员用政策梯度更新评论家指导的政策概率分布。随着时间的推移，演员学会采取更好的行动，评论家会更好地评估这些行动。演员-评论家方法已被证明能够学习和适应大型复杂的环境。因此，演员-评论家方法非常适合与大型股票投资组合进行交易

累计收益：由投资组合的初始值减去投资组合的最终值，然后除以初始值来计算；

年化回报：是代理人在一段时间内每年赚取的几何平均金额。
年化波动率：是投资组合收益的年化标准差；

夏普比率：通过从年化收益中减去年化无风险利率，再除以年化波动率计算得出；

最大缩水：最交易期间的最大百分比损失。

https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020

问题：
论文没有实验过程，只有算法理论，最后给出实验结果。
投资组合价值

使用基于行为-批评的算法来学习股票交易策略的潜力，这些算法包括近端策略优化（PPO）、优势行为-批评（A2C）和深度确定性策略梯度（DDPG）代理。为了适应不同的市场情况，我们使用集成策略，根据夏普比率自动选择表现最好的代理进行交易。结果表明，我们的集成策略通过在交易成本下平衡风险和收益，在夏普比率方面优于三种单独的算法，即道琼斯工业平均法和最小方差投资组合分配法。
### 夏普比率
$ \text { SharpeRatio }=\frac{E\left(R_{P}\right)-R_{f}}{\sigma_{P}} $

Rf:年化无风险利率

σp:投资组合年化报酬率的标准差

假如国债的回报是3%，而你的投资组合预期回报是15%，你的投资组合的标准偏差是6%，那么用15%－3%,可以得出12%（代表超出无风险投资的回报），再用12%/6%=2，代表投资者风险每增长1%，换来的是2%的多余收益

夏普理论告诉我们，投资时也要比较风险，尽可能用科学的方法以冒小风险来换大回报。所以说，投资者应该成熟起来，尽量避免一些不值得冒的风险。同时在投资时如缺乏投资经验与研究时间，可以让真正的专业人士（不是只会卖金融产品给你的SALER）来帮到你建立起适合自己的，可承受风险最小化的投资组合。这些投资组合可以通过Sharpe Ratio来衡量出风险和回报比例
