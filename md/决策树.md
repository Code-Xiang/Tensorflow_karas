## 决策树

决策树可解决的问题：

分类(classification)和回归(regression)两种问题。

贪心算法__决策树是自顶向下递归的方式，每一步采用当前下的最优选择。构造决策树的关键是选择一个好的分裂标准。

ID3使用信息增益的为特征选择的量度。

衡量系统的程度---熵

假设随机变量X的取值为{x1,x2...xn},对应出现概率为{P1,P2...Pn}。X的信息熵用来衡量系统混乱程度表示为：

$H(X)=-\sum_{i=1}^{n} p_{i} \log _{2} p_{i}$

均匀分布(uniform distribution)有着最大的信息熵($f(x)=\frac{1}{b-a}, a<x<b$)

基尼系数也用来衡量系统的混乱程度

$G(X)=1-\sum_{i=1}^{n} p_{i}^{2}$



##### 信息增益：得知特征A的信息而使得样本集合不确定性(熵)减少程度

$ \operatorname{Gain}(S, A) \equiv \underline{\operatorname{Entropy}S} 
 \sum_{v \in \operatorname{Values}(A)} \frac{\left|S_{v}\right|}{|S|} \operatorname{Entropy}\left(S_{v}\right) 
$

![image-20220427095900190](/Users/xiangxuewei/Library/Application Support/typora-user-images/image-20220427095900190.png)



#### 终止条件：

节点终止分裂的条件：如果所有样例点被完美地分类/符合终止条件，则RETURN

完美分类：

- 如果所有的当前子集中的数据有相同类的输出，然后就stop（足够纯净）
- 如果所有的当前子集中的数据有相同输入值，然后就stop（必须随机分配一个值）

实际应用中，我们会采取更宽松的停止条件。

#### 树的拟合和剪枝(pruning)

树很容易过拟合是一个非常严重的问题。在极端情况，在每个叶子节点可能只有一个元素，这样整棵树变成了一个look-up table.这样说树几乎没有泛化能力！

两种解决决策树的过拟合问题：pre-pruning和post-pruning

1）pre-pruning：当节点分类带来的收益不统计学上显著（statistically significant），停止分裂

- 方法1:如果到达该节点的样本数<总样本数*5%，则停止分裂。因为数据集过小会导致方差增大，从而导致范化性变差。
- 方法2:如果该节点分裂的信息增益<threshold,则停止分裂。

2）post-pruning：先生成整棵树，然后后剪枝

- 方法1--减少误差的后剪枝方法(reduced-error post-pruning):把数据集分为训练集和验证集，尝试剪掉每一个节点并测试在验证集的结果。每次都贪心的剪掉那个带来最大验证集上准确率提升的节点。
- 方法2--规则后剪枝（rule post-pruning）：

