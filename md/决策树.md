## 决策树

决策树可解决的问题：

分类(classification)和回归(regression)两种问题。

贪心算法__决策树是自顶向下递归的方式，每一步采用当前下的最优选择。构造决策树的关键是选择一个好的分裂标准。

##### ID3使用信息增益的为特征选择的量度。

衡量系统的程度---熵

假设随机变量X的取值为{x1,x2...xn},对应出现概率为{P1,P2...Pn}。X的信息熵用来衡量系统混乱程度表示为：

$H(X)=-\sum_{i=1}^{n} p_{i} \log _{2} p_{i}$

均匀分布(uniform distribution)有着最大的信息熵($f(x)=\frac{1}{b-a}, a<x<b$)

基尼系数也用来衡量系统的混乱程度

$G(X)=1-\sum_{i=1}^{n} p_{i}^{2}$



##### 信息增益：得知特征A的信息而使得样本集合不确定性(熵)减少程度

$ \operatorname{Gain}(S, A) \equiv \underline{\operatorname{Entropy}S} 
 \sum_{v \in \operatorname{Values}(A)} \frac{\left|S_{v}\right|}{|S|} \operatorname{Entropy}\left(S_{v}\right) 
$

![image-20220427095900190](/Users/xiangxuewei/Library/Application Support/typora-user-images/image-20220427095900190.png)



#### 终止条件：

节点终止分裂的条件：如果所有样例点被完美地分类/符合终止条件，则RETURN

完美分类：

- 如果所有的当前子集中的数据有相同类的输出，然后就stop（足够纯净）
- 如果所有的当前子集中的数据有相同输入值，然后就stop（必须随机分配一个值）

实际应用中，我们会采取更宽松的停止条件。

#### 树的拟合和剪枝(pruning)

树很容易过拟合是一个非常严重的问题。在极端情况，在每个叶子节点可能只有一个元素，这样整棵树变成了一个look-up table.这样说树几乎没有泛化能力！

两种解决决策树的过拟合问题：pre-pruning和post-pruning

1）pre-pruning：当节点分类带来的收益不统计学上显著（statistically significant），停止分裂

- 方法1:如果到达该节点的样本数<总样本数*5%，则停止分裂。因为数据集过小会导致方差增大，从而导致范化性变差。
- 方法2:如果该节点分裂的信息增益<threshold,则停止分裂。

2）post-pruning：先生成整棵树，然后后剪枝

- 方法1--减少误差的后剪枝方法(reduced-error post-pruning):把数据集分为训练集和验证集，尝试剪掉每一个节点并测试在验证集的结果。每次都贪心的剪掉那个带来最大验证集上准确率提升的节点。

- 方法2--规则后剪枝（rule post-pruning）：

  1、将树转换成规则的集

  2、通过删除任何先决条件来删除每条规则，以提高其估计精度(prune each rule by removing any preconditions that result in impoving its estimated accurcy)

  3、将规则按所需顺序排序(按其估计精度)

  4、对实例进行分类时，按照相同的顺序使用最后的规则

##### **ID3算法的缺点**

- 信息增益准则对可取值数目较多的特征有所偏好(信息增益反映的给定一个条件以后不确定性减少的程度,必然是分得越细的数据集确定性更高)
- ID3算法没有进行决策树剪枝，容易发生过拟合
- ID3算法没有给出处理连续数据的方法，只能处理离散特征
- ID3算法不能处理带有缺失值的数据集,需对数据集中的缺失值进行预处理

##### C4.5 by Quinlan（对ID3算法进行了改进）

- 使用信息增益比代替信息增益【ID3中的信息增益其实是有bias的：对于那些特别多取值的特征(e.g. date,id)，信息增益自然就会很大。极端情况，如果直接用id作为一个特征，那么信息增益是很大的，因为按照id划分之后，节点不纯度变成了0！

  <img src="/Users/xiangxuewei/Library/Application Support/typora-user-images/image-20220427160715065.png" alt="image-20220427160715065" style="zoom:50%;" />

  信息增益比对那些属性值特别多的属性(如date,id)做了惩罚。

  但是，信息增益比对**取值较少的特征**有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用信息增益比最大的特征进行划分，而是使用一个启发式方法：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择信息增益比最高的】

- 使用后剪枝【在决策树构造完成后，自底向上对非叶节点进行评估，如果将其换成叶节点能提升泛化性能，则将该子树换成叶节点。后剪枝决策树欠拟合风险很小，泛化性能往往优于预剪枝决策树。但训练时间会大很多。】

- 增加处理连续特征的方法--连续特征离散化【需要处理的样本或样本子集按照连续变量的大小**从小到大进行排序**、

  假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的**中点**,根据这个分割点把原来连续的属性分成两类】

- 缺失值处理【对于具有缺失值的特征，用没有缺失的样本子集所占比重来折算信息增益率，选择划分特征、

  选定该划分特征，对于缺失该特征值的样本，将样本以不同的概率划分到不同子节点】

C4.5的缺点

- C4.5只能用于分类
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行**排序**，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。而且，由于需要对数据集进行多次的顺序扫描和排序，算法低效。

#### CART算法（classification and regression）

相比ID3和C4.5算法，CART算法使用**二叉树**简化决策树规模，提高生成决策树效率。CART决策树生成分为**分类树**和**回归树**两种场景，两者生成方式有一定的区别。

CART分类树在节点分裂时使用**GINI指数**来替代信息增益。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。我们按照分裂前后基尼指数的差值，找到最好的分裂准则以及分裂值，将根节点一分为二。

##### CART算法优点

- CART使用**二叉树**来代替C4.5的多叉树，提高了生成决策树效率
- C4.5只能用于分类，CART树可用于**分类和回归(C**lassification **A**nd **R**egression **T**ree**)**
- CART 使用 **Gini 系数**作为变量的不纯度量，减少了大量的对数运算
- ID3 和 C4.5 只使用一次特征，CART 可多次**重复使用特征**

[【预估排序】Xgboost、GBDT、CART等树模型联系和区别（超级详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/158633779)
