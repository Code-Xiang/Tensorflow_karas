## 决策树

决策树可解决的问题：

分类(classification)和回归(regression)两种问题。

贪心算法__决策树是自顶向下递归的方式，每一步采用当前下的最优选择。构造决策树的关键是选择一个好的分裂标准。

##### ID3使用信息增益的为特征选择的量度。

衡量系统的程度---熵

假设随机变量X的取值为{x1,x2...xn},对应出现概率为{P1,P2...Pn}。X的信息熵用来衡量系统混乱程度表示为：

$H(X)=-\sum_{i=1}^{n} p_{i} \log _{2} p_{i}$

均匀分布(uniform distribution)有着最大的信息熵($f(x)=\frac{1}{b-a}, a<x<b$)

基尼系数也用来衡量系统的混乱程度

$G(X)=1-\sum_{i=1}^{n} p_{i}^{2}$



##### 信息增益：得知特征A的信息而使得样本集合不确定性(熵)减少程度

$ \operatorname{Gain}(S, A) \equiv \underline{\operatorname{Entropy}S} 
 \sum_{v \in \operatorname{Values}(A)} \frac{\left|S_{v}\right|}{|S|} \operatorname{Entropy}\left(S_{v}\right) 
$

![image-20220427095900190](/Users/xiangxuewei/Library/Application Support/typora-user-images/image-20220427095900190.png)



#### 终止条件：

节点终止分裂的条件：如果所有样例点被完美地分类/符合终止条件，则RETURN

完美分类：

- 如果所有的当前子集中的数据有相同类的输出，然后就stop（足够纯净）
- 如果所有的当前子集中的数据有相同输入值，然后就stop（必须随机分配一个值）

实际应用中，我们会采取更宽松的停止条件。

#### 树的拟合和剪枝(pruning)

树很容易过拟合是一个非常严重的问题。在极端情况，在每个叶子节点可能只有一个元素，这样整棵树变成了一个look-up table.这样说树几乎没有泛化能力！

两种解决决策树的过拟合问题：pre-pruning和post-pruning

1）pre-pruning：当节点分类带来的收益不统计学上显著（statistically significant），停止分裂

- 方法1:如果到达该节点的样本数<总样本数*5%，则停止分裂。因为数据集过小会导致方差增大，从而导致范化性变差。
- 方法2:如果该节点分裂的信息增益<threshold,则停止分裂。

2）post-pruning：先生成整棵树，然后后剪枝

- 方法1--减少误差的后剪枝方法(reduced-error post-pruning):把数据集分为训练集和验证集，尝试剪掉每一个节点并测试在验证集的结果。每次都贪心的剪掉那个带来最大验证集上准确率提升的节点。

- 方法2--规则后剪枝（rule post-pruning）：

  1、将树转换成规则的集

  2、通过删除任何先决条件来删除每条规则，以提高其估计精度(prune each rule by removing any preconditions that result in impoving its estimated accurcy)

  3、将规则按所需顺序排序(按其估计精度)

  4、对实例进行分类时，按照相同的顺序使用最后的规则

##### **ID3算法的缺点**

- 信息增益准则对可取值数目较多的特征有所偏好(信息增益反映的给定一个条件以后不确定性减少的程度,必然是分得越细的数据集确定性更高)
- ID3算法没有进行决策树剪枝，容易发生过拟合
- ID3算法没有给出处理连续数据的方法，只能处理离散特征
- ID3算法不能处理带有缺失值的数据集,需对数据集中的缺失值进行预处理

##### C4.5 by Quinlan（对ID3算法进行了改进）

- 使用信息增益比代替信息增益【ID3中的信息增益其实是有bias的：对于那些特别多取值的特征(e.g. date,id)，信息增益自然就会很大。极端情况，如果直接用id作为一个特征，那么信息增益是很大的，因为按照id划分之后，节点不纯度变成了0！

  <img src="/Users/xiangxuewei/Library/Application Support/typora-user-images/image-20220427160715065.png" alt="image-20220427160715065" style="zoom:50%;" />

  信息增益比对那些属性值特别多的属性(如date,id)做了惩罚。

  但是，信息增益比对**取值较少的特征**有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用信息增益比最大的特征进行划分，而是使用一个启发式方法：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择信息增益比最高的】

- 使用后剪枝【在决策树构造完成后，自底向上对非叶节点进行评估，如果将其换成叶节点能提升泛化性能，则将该子树换成叶节点。后剪枝决策树欠拟合风险很小，泛化性能往往优于预剪枝决策树。但训练时间会大很多。】

- 增加处理连续特征的方法--连续特征离散化【需要处理的样本或样本子集按照连续变量的大小**从小到大进行排序**、

  假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的**中点**,根据这个分割点把原来连续的属性分成两类】

- 缺失值处理【对于具有缺失值的特征，用没有缺失的样本子集所占比重来折算信息增益率，选择划分特征、

  选定该划分特征，对于缺失该特征值的样本，将样本以不同的概率划分到不同子节点】

C4.5的缺点

- C4.5只能用于分类
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行**排序**，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。而且，由于需要对数据集进行多次的顺序扫描和排序，算法低效。

#### CART算法（classification and regression）

相比ID3和C4.5算法，CART算法使用**二叉树**简化决策树规模，提高生成决策树效率。CART决策树生成分为**分类树**和**回归树**两种场景，两者生成方式有一定的区别。

CART分类树在节点分裂时使用**GINI指数**来替代信息增益。基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。我们按照分裂前后基尼指数的差值，找到最好的分裂准则以及分裂值，将根节点一分为二。

##### CART算法优点

- CART使用**二叉树**来代替C4.5的多叉树，提高了生成决策树效率
- C4.5只能用于分类，CART树可用于**分类和回归(C**lassification **A**nd **R**egression **T**ree**)**
- CART 使用 **Gini 系数**作为变量的不纯度量，减少了大量的对数运算
- ID3 和 C4.5 只使用一次特征，CART 可多次**重复使用特征**

[【预估排序】Xgboost、GBDT、CART等树模型联系和区别（超级详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/158633779)

#### 决策树生成

CART决策树生成分为 分类树和回归树两种场景。生成方式有一定的区别

###### *分类树*

CART分类树：特征属性是连续类型的也可以是离散类型，但是观察属性（即标签属性或着分类属性）必须是离散类型。

CART分类树在节点分裂时使用GINI指数。基尼指数代表模型的不纯度，基尼系数越小，不纯度就越低，特征越好。这和信息增益（率）正好相反。

$\begin{aligned}
\operatorname{Gini}(D) &=\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|}\left(1-\frac{\left|C_{k}\right|}{|D|}\right) \\
&=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2} \operatorname{Gini}(D \mid A) \\
&=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \operatorname{Gini}\left(D_{i}\right)
\end{aligned}$

其中K代表类别。

#### **离散值和连续值处理**

对于离散属性，可能会出现属性取值数N>=3的情况，因为CART是二叉树，此时需要考虑将N>=3个取值的离散特征的处理时也只能有两个分支，这就要通过组合人为的创建二取值序列并取GiniGain最小者作为树分叉决策点。

对于连续值，CART 分类树采用基尼系数的大小来度量特征的各个划分点。

##### *缺失值处理*

CART 算法使用一种惩罚机制来抑制提升值，从而反映缺失值的影响。为树的每个节点都找到代理分裂器，当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。

#### 回归树

回归树要求观察属性是连续类型，由于节点分裂选择特征属性时通常使用最小绝对偏差（LAD）或者最小二乘偏差（LSD）法，因此通常特征属性也是连续类型。

对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集 ![[公式]](https://www.zhihu.com/equation?tex=D_1) 和 ![[公式]](https://www.zhihu.com/equation?tex=D_2) ，求出使 ![[公式]](https://www.zhihu.com/equation?tex=D_1) 和 ![[公式]](https://www.zhihu.com/equation?tex=D_2) 各自集合的均方差最小，同时 ![[公式]](https://www.zhihu.com/equation?tex=D_1) 和 ![[公式]](https://www.zhihu.com/equation?tex=D_2) 的均方差之和最小所对应的特征和特征值划分点。表达式为

$\min _{a, s}\left[\min _{c_{1}} \sum_{x_{i} \in D_{1}}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in D_{2}}\left(y_{i}-c_{2}\right)^{2}\right]$

其中， ![[公式]](https://www.zhihu.com/equation?tex=c_1) 为 ![[公式]](https://www.zhihu.com/equation?tex=D_1) 数据集的样本输出均值， ![[公式]](https://www.zhihu.com/equation?tex=c_2) 为 ![[公式]](https://www.zhihu.com/equation?tex=D_2) 数据集的样本输出均值。

对于决策树建立后做预测的方式，CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。

#### 剪枝策略

采用一种“基于代价复杂度的剪枝”方法进行后剪枝。具体做法为：

我们将一颗充分生长的树称为T0 ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下：

$C_{\alpha}(T)=C(T)+\alpha|T|$

![[公式]](https://www.zhihu.com/equation?tex=T) 为任意子树， ![[公式]](https://www.zhihu.com/equation?tex=C%28T%29) 为预测误差， ![[公式]](https://www.zhihu.com/equation?tex=%7CT%7C) 为子树 ![[公式]](https://www.zhihu.com/equation?tex=T) 的叶子节点个数， ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 是参数， ![[公式]](https://www.zhihu.com/equation?tex=C%28T%29) 衡量训练数据的拟合程度， ![[公式]](https://www.zhihu.com/equation?tex=%7CT%7C) 衡量树的复杂度， ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 权衡拟合程度与树的复杂度。

那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)

- 当α很小的时候，T0 是这样的最优子树
- 当α很大的时候，单独一个根节点就是最优子树

尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行交叉验证，找到最优的那个子树作为我们的决策树。因此CART剪枝分为两部分，分别是**生成子树序列**和**交叉验证**。

### Bagging 算法和Boosting算法

Bagging和Boosting区别

Bagging算法和Boosting都属于集成算法，最重要的假设是：当弱模型被正确组合时，我们可以得到更精确和/或更鲁棒的模型。

- bagging算法通常考虑的是同质弱学习器，相互独立地并行学习这些弱学习器，并按照某种确定性的平均过程将它们组合起来。
- boosting算法通常考虑的也是同质弱学习器。它以一种高度自适应的方法顺序地学习这些弱学习器（每个基础模型都依赖于前面的模型），并按照某种确定性的策略将它们组合起来。

bagging 的重点在于获得一个**方差**比其组成部分更小的集成模型，而 boosting 和 stacking 则将主要生成**偏差**比其组成部分更低的强模型（即使方差也可以被减小）

**Bagging和Boosting 的主要区别**

- Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是每个样本的权重。
- Bagging采取的是均匀取样，每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重越大
- Bagging所有预测函数权值相同，Boosting中误差越小的预测函数权值越大。
- Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成

将Bagging和Boosting分别和树模型结合分别生成：

- Bagging+决策树=随机森林
- Boosting+决策树=GBDT
